---
---

@string{aps = {American Physical Society,}}

@article{xu2023delegation,
  selected={true},
  abbr={Working Paper},
  title={Learning from a Mixture of Information Sources},
  abstract={We often receive information from multiple sources, but with varying levels of reliability or precision. For example, consumers read reviews from those with tastes like theirs, or polar opposite to them. Policies are shaped by a mix of opinions, some from experts, others from laypersons. LLM tools gather data from countless websites, some reputable, others dubious. We ask: is it important to know where each piece of information comes from? What properties of the information space makes the context especially helpful? This paper tries to provide both a tool and an answer for such questions.},
  author={ Immorlica, Nicole and Lucier, Brendan and Thomas, Clayton and Xu, Ruqing},
  journal={Working paper},
  year={2025}
}

@article{xu2023delegation,
  selected={true},
  abbr={arXiv},
  title={Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions},
  abstract={A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1)  Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent's information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm's prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Well-intentioned policies aiming to provide more information, such as keeping a ``human-in-the-loop'' or requiring maximal prediction accuracy, could strictly worsen decision quality compared to systems with no human or no algorithmic assistance. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.},
  author={Xu, Ruqing},
  journal={Working paper},
  year={2024},
  arxiv={2402.09384},
  pdf={delegation_xu.pdf}
}

@article{enem2023,
  selected={true},
  abbr={Working Paper},
  title={Stakes and Signals: An Empirical Investigation of Muddled Information in Standardized Testing},
  abstract={We examine a natural experiment in Brazil in which similar students took the same standardized test as either a low-stakes school accountability exam or a high- stakes admission exam for the country's top universities. Using administrative data and a difference-in-differences design, we find that test score gaps between high- and low-income students expanded on the high-stakes exam, consistent with wealthy students engaging in test prep. Yet the increase in stakes made scores more informative for students' college outcomes. Thus the ``muddling'' of information on natural ability and test prep improved the quality of the score signal, although it also exacerbated inequality.},
  author={Reyes, Germ\'{a}n and Riehl, Evan and Xu, Ruqing},
  journal={NBER Working paper},
  year={2024},
  pdf={rrx_stakes_june2024.pdf}
}





